{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finding Method and Sample Size in Clinical Documents.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MyXVJmnpAG9J"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik9ygbzEPq4Y",
        "colab_type": "text"
      },
      "source": [
        "#Finding Attributes of Clinical Research Documents\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Technologies: Logistical Regression Classifier on Sentence Embedding to Spacy tokenization to Word2vec for Sample Size extraction to number. **\n",
        "\n",
        "\n",
        "\n",
        "This document will go into a step by step process for training a logistical regression classifier to find the sentences and parse throug the sentence to determine the best match exact values for the specific clinical research document attribute. Please note this repo is part of a larger ETL process and is only for getting simple attributes about the study as a whole and not full processing of extended datasets within the study that are needed. A seperate processing script would be used utilizing the information here in further processing. \n",
        "\n",
        "Ex - Finding all patients within the document and splitting the data by patient in order to more accurately map conditions, symptoms, risk factors/circumstances, and treatments for a more accurate representation if the structured data is not available or does not show all the observations. \n",
        "\n",
        "\n",
        "Current Attributes Supported:\n",
        "1.   Sample Size\n",
        "1.   Sample Methodology\n",
        "2.   Method\n",
        "\n",
        "Future Support:\n",
        "\n",
        "1.   Topics/Drug/Condition Topic \n",
        "2.   Recruiting Periods \n",
        "3. Researchers - in case not in Meta data for Author\n",
        "4. Countries Included \n",
        "\n",
        "Items Already in Meta:\n",
        "1. Authors \n",
        "2. Title \n",
        "3. Citations \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n41BAXbQ_kq",
        "colab_type": "text"
      },
      "source": [
        "#Initial Installations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmUElyWAPV9d",
        "colab_type": "code",
        "outputId": "f3e81eee-c1dc-4856-d679-f73183cfa3cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "#Load everything \n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz\n",
        "!pip install word2number\n",
        "!Pip install spacy\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import requests\n",
        "import zipfile\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz\n",
            "\u001b[?25l  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz (70.0MB)\n",
            "\u001b[K     |████████████████████████████████| 70.0MB 53kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from en-core-sci-md==0.2.4) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (1.18.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (46.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (4.38.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-md==0.2.4) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-md==0.2.4) (1.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-md==0.2.4) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-md==0.2.4) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-md==0.2.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-md==0.2.4) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-md==0.2.4) (3.1.0)\n",
            "Building wheels for collected packages: en-core-sci-md\n",
            "  Building wheel for en-core-sci-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-sci-md: filename=en_core_sci_md-0.2.4-cp36-none-any.whl size=70498247 sha256=c0d2a0d42936b65132d5f406757aecda272b27ef0bd45b61be7c55720070dcdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/b3/89/7fbb30f56411e8b4002eac6d5568ab46da63191a2287aa17bf\n",
            "Successfully built en-core-sci-md\n",
            "Installing collected packages: en-core-sci-md\n",
            "Successfully installed en-core-sci-md-0.2.4\n",
            "Collecting word2number\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=5e83471f0b9afc04c9f1c5b2d59635f08967830e3922850fbf6c982608db3f8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n",
            "/bin/bash: Pip: command not found\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjkCADdPQl2k",
        "colab_type": "text"
      },
      "source": [
        "#Load the training data into the environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-QpAmZQcK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    os.stat(\"./data/\")\n",
        "except:\n",
        "    os.mkdir(\"./data/\")  \n",
        "url = 'https://github.com/Deamoner/annotated-clinical-research-study-attribtes-data/archive/master.zip'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('./data/design-data.zip', 'wb').write(r.content)\n",
        "#Now unzip the file \n",
        "with zipfile.ZipFile('./data/design-data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./data/')\n",
        "#Move the data manually to the data folder - Testing if your paying attention\n",
        "# attribute.csv is the only file currently being processed. \n",
        "# Note press refresh on the files tab to see the new directories and files to move. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8tSWpKpSnh0",
        "colab_type": "text"
      },
      "source": [
        "#Load the data in a dataframe for access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNcVh3wSSt_8",
        "colab_type": "code",
        "outputId": "1b8d24d0-8cab-4047-e900-0a764c6db910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv(\"./data/attribute.csv\", encoding = \"ISO-8859-1\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>other</th>\n",
              "      <th>method</th>\n",
              "      <th>sample</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>'Epidemiology' includes studies on the epidemi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(3) Approved the final version of the manuscri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[17] Further researches are needed to investig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[2] [3, 4] It disproportionately affects the e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[30] It has been conjectured the loss of funct...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   other  method  sample                                               text\n",
              "0      0       0       0  'Epidemiology' includes studies on the epidemi...\n",
              "1      0       0       0  (3) Approved the final version of the manuscri...\n",
              "2      0       0       0  [17] Further researches are needed to investig...\n",
              "3      0       0       0  [2] [3, 4] It disproportionately affects the e...\n",
              "4      0       0       0  [30] It has been conjectured the loss of funct..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHkUS0LnXr3W",
        "colab_type": "text"
      },
      "source": [
        "#Train the classifier in a pipeline \n",
        "\n",
        "Split the data into train test split. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8z0XdHPXy7N",
        "colab_type": "code",
        "outputId": "ee0df587-e2b7-4544-d682-66de33ea19be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train.text\n",
        "X_test = test.text\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2682,)\n",
            "(1322,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jof11_MaX199",
        "colab_type": "code",
        "outputId": "bc3a954e-127e-4106-9311-4257483010b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Define a pipeline combining a text feature extractor with multi lable classifier\n",
        "#You will notice that I have chosen to only train one category at this point. \n",
        "#the for loop on categories will retrain over the NB_pipeline model.\n",
        "#In future will need to turn into array of models or redo the multiclassification in one model. \n",
        "categories = ['sample']\n",
        "NB_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    NB_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = NB_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing sample\n",
            "Test accuracy is 0.9046898638426626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXK3Hr8cChKo",
        "colab_type": "text"
      },
      "source": [
        "#Model Saving and Loading\n",
        "This section is important for production implementation scenarios. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--AF1fKHwBqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the model \n",
        "\n",
        "import pickle\n",
        "filename = \"abc.sav\"\n",
        "pickle.dump(NB_pipeline, open(filename, 'wb'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpw4X_cgyk9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the model\n",
        "load_model = pickle.load(open(filename,'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGwjHkGUCudO",
        "colab_type": "text"
      },
      "source": [
        "#Simple Interfence Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E44Ty5XzywUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Inference Demo\n",
        "#One Sentence Test of Prediction\n",
        "sentence = \"\tMethods: From January 20 to February 5, 2020, a total of 130 patients diagnosed with COVID-19 from seven hospitals in China were collected.\"\n",
        "#All data preidiction accuracy \n",
        "testsentences=pd.Series(df.text[0:1000])\n",
        "sseries = pd.Series(sentence)\n",
        "#Run Predictions \n",
        "prediction_all= load_model.predict(testsentences)\n",
        "predict_one = load_model.predict(sseries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aBckYfK35ZL",
        "colab_type": "code",
        "outputId": "11da9944-935c-492e-e84f-ad79cc071a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "#Print all to view all predictions - Good Practice as the accuracy reported never matches\n",
        "prediction_all\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmQ6Tu6J-kIu",
        "colab_type": "code",
        "outputId": "2f453c6a-3dac-4ec6-92b1-e81c6028cd42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Print one prediction\n",
        "predict_one[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUscJozBZHO3",
        "colab_type": "text"
      },
      "source": [
        "#Review\n",
        "\n",
        "\n",
        "\n",
        "These are a pretty good starting point. Upon further review I think it is saying it gets 90% accuracy accross all predictions, yet the subset that are sample I think has a number of false positives and an equal number of false negatives. \n",
        "\n",
        " \n",
        "2.   Testing with NLP Tags from Spacy added to the vectors \n",
        "3. Further NLP Process to extract exact information - This is from the sample script for later. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKtZmZ8q7-pT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_all\n",
        "df.text[0:1000]\n",
        "#Test out a loop for all the predictions and review for where the error\n",
        "  if prediction_all[i]:\n",
        "    count = count + 1\n",
        "    #print(prediction_all[i])\n",
        "    print(testsentences[i])\n",
        "    extracted = extracted = extract(testsentences[i],\"sample\")\n",
        "    print(extracted)\n",
        "    print(\"-----------------------\")\n",
        "print(\"Total:\")\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyXVJmnpAG9J",
        "colab_type": "text"
      },
      "source": [
        "#Bad Performance \n",
        "Seems like performance on the training results are optimistic. It says high 80's, but when you re-run the entire series through it then the predictions are mostly wrong. On first try the sentences were not tagging properly at all with a less then 2% chance it would hit out of the thousand. \n",
        "What did I do?\n",
        "Double the data of the same - It should be able to tag it'self properly at the very minimum. If it can't tag it's self it must not have enough of that example data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6wzbJUAM_Q",
        "colab_type": "code",
        "outputId": "9a56034c-a4bc-43dd-c14f-c7bda9b70298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#Double the data and retrain \n",
        "newdf = pd.read_csv(\"./data/attribute.csv\", encoding = \"ISO-8859-1\")\n",
        "newdf.head()\n",
        "bigdf = [df, newdf]\n",
        "result = pd.concat(bigdf)\n",
        "bigdf = [result, newdf]\n",
        "result = pd.concat(bigdf)\n",
        "result = pd.concat(bigdf)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>other</th>\n",
              "      <th>method</th>\n",
              "      <th>sample</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>'Epidemiology' includes studies on the epidemi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(3) Approved the final version of the manuscri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[17] Further researches are needed to investig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[2] [3, 4] It disproportionately affects the e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[30] It has been conjectured the loss of funct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Three-hundred and fifty-five COVID-19 patients...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Using data from the Truven Health MarketScan R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>We collected data from a sample of 235 adult p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>We describe two patients with no underlying di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>We used observational healthcare data on 1559 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2994 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     other  method  sample                                               text\n",
              "0        0       0       0  'Epidemiology' includes studies on the epidemi...\n",
              "1        0       0       0  (3) Approved the final version of the manuscri...\n",
              "2        0       0       0  [17] Further researches are needed to investig...\n",
              "3        0       0       0  [2] [3, 4] It disproportionately affects the e...\n",
              "4        0       0       0  [30] It has been conjectured the loss of funct...\n",
              "..     ...     ...     ...                                                ...\n",
              "993      0       1       1  Three-hundred and fifty-five COVID-19 patients...\n",
              "994      0       1       1  Using data from the Truven Health MarketScan R...\n",
              "995      0       1       1  We collected data from a sample of 235 adult p...\n",
              "996      0       1       1  We describe two patients with no underlying di...\n",
              "997      0       1       1  We used observational healthcare data on 1559 ...\n",
              "\n",
              "[2994 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gicDMNdFjwM",
        "colab_type": "code",
        "outputId": "ebddd6f6-433f-416b-d52d-540eb2917696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train, test = train_test_split(result, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train.text\n",
        "X_test = test.text\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2005,)\n",
            "(989,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krqYIeJRFbyf",
        "colab_type": "code",
        "outputId": "9a7e6581-8052-4be3-ad6e-9b1b7b40c186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "categories = ['method']\n",
        "retrain_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    retrain_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = retrain_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing method\n",
            "Test accuracy is 0.916076845298281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U6EbjsQGUCu",
        "colab_type": "code",
        "outputId": "3287852c-0cca-410d-a016-c60cbb46f3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Training accuracy went up, but do we now match closer to the training file for all predicictions? Take a closer look at the output and determine for yourself.\n",
        "#Inference \n",
        "#One Sentence Test of Prediction\n",
        "sentence = \"We collected data from a sample of 235 adult patients from the Hospital Israelita Albert Einstein in Sao Paulo, Brazil, from 17 to 30 of March, 2020, of which 102 (43%) received a positive diagnosis of COVID-19 from RT-PCR tests.\"\n",
        "#All data preidiction accuracy \n",
        "testsentences=pd.Series(result.text)\n",
        "sseries = pd.Series(sentence)\n",
        "#Run Predictions \n",
        "prediction_all= retrain_pipeline.predict(testsentences)\n",
        "predict_one = retrain_pipeline.predict(sseries)\n",
        "prediction_all"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXLYHm-bpOif",
        "colab_type": "code",
        "outputId": "e93c8b8e-8b27-4964-f38d-3922659a469b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_one[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWPo8w2yH4Ci",
        "colab_type": "text"
      },
      "source": [
        "#Other Experiments in classification accuracy increase \n",
        "There will be a number of ways for increasing the accuracy. Here we will experiment a little to see what works. \n",
        "\n",
        "What are the options?\n",
        "\n",
        "\n",
        "1.   Add more relevant features into the vectors - spacy model tokens pretrained on medical corpuse \n",
        "2.   Review where the issues are in the current predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6TuekZq5tRj",
        "colab_type": "text"
      },
      "source": [
        "#Analysis of results \n",
        "Still to be done \n",
        "f1 score \n",
        "roc\n",
        "confusion matrix \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5dUd-UkJNfD",
        "colab_type": "text"
      },
      "source": [
        "#Further NLP Extraction on Classified Sentences  \n",
        "What needs to be done to make these usable and get the exact data needed?\n",
        "\n",
        "\n",
        "1.   Get all predictions tagged with text and predictions \n",
        "1.   Find best preidictions per topic on specific predictions. IE best sample size sentence by prediction. \n",
        "1.   Load in a Vocab object to be used for further parsing with all pertinent terms. \n",
        "1.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqwLV_p0Wi9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's define some Vocab objects to be used \n",
        "#Credit to https://www.kaggle.com/savannareid for providing keywords and analysis.\n",
        "class Vocab(object):\n",
        "    \"\"\"\n",
        "    Defines vocabulary terms for studies.\n",
        "    \"\"\"\n",
        "\n",
        "    # Study titles\n",
        "    TITLE = [r\"case (?:series|study)\", r\"cross[\\-\\s]?sectional\", r\"mathematical model(?:ing)?\", \"meta-analysis\",\n",
        "             r\"non[\\-\\s]randomized\", \"prospective cohort\", \"randomized\", \"retrospective cohort\", \"systematic review\",\n",
        "             r\"time[\\-\\s]?series\"]\n",
        "\n",
        "    # Study design vocabulary\n",
        "    DESIGN = [r\"(?:electronic )?health records\", r\"(?:electronic )?medical records\", \"adherence\", \"adjusted hazard ratio\",\n",
        "              \"adjusted odds ratio\", \"ahr\", \"aic\", \"akaike information criterion\", \"allocation method\", \"allocation method double-blind\",\n",
        "              \"aor\", \"area under the curve\", \"associated with\", \"associated with random sample\", \"association\", \"attack rate\", \"auc\"\n",
        "              \"baseline\", \"blind\", \"bootstrap\", \"bootstrap auc\", \"case report\", \"case report clinical findings\", \"case series\",\n",
        "              \"case study\", r\"case[\\-\\s]control\", \"censoring\", \"chart review\", \"cochrane review\", \"coefficient\", \"cohen's d\", \"cohen's kappa\",\n",
        "              \"cohort\", r\"computer model(?:ing)?\", \"confounding\", \"consort\", \"control arm\", r\"correlation(?:s)?\",\n",
        "              \"covariates\", \"cox proportional hazards\", \"cross-sectional survey\", r\"cross[\\-\\s]sectional\", \"d-pooled\", \"data abstraction forms\",\n",
        "              \"data collection instrument\", r\"database(?:s)? search(?:ed)?\", \"databases searched\", r\"deep[\\-\\s]learning\", \"demographics\", \"diagnosis\",\n",
        "              \"difference between means\", \"difference in means\", \"dosage\", \"double-blind\", \"duration\", \"editor\", \"ehr\", \"electronic health records\",\n",
        "              \"electronic search\", \"eligib(?:e|ility)\", \"eligibility criteria\", r\"enroll(?:ed|ment)?\", \"estimation\", \"etiology\", \"exclusion criteria\",\n",
        "              \"exposure status\", \"follow-up\", \"followed\", r\"forecast(?:ing)?\", \"frequency\", \"gamma\", \"hazard ratio\", \"heterogeneity\", \"hr\", \"i2\",\n",
        "              \"incidence\", \"inclusion criteria\", \"inter-rater reliability\", \"interrater reliability\", \"interventions\", \"kaplan-meier\", \"log odds\",\n",
        "              \"logistic regression\", \"lognormal\", \"longitudinal\", \"loss to follow-up\", r\"machine[\\-\\s]learning\", r\"match(?:ed|ing)? case\",\n",
        "              r\"match(?:ed|ing)? criteria\", \"matched\", \"matching\", r\"mathematical model(?:ing)?\", \"mean difference\", \"median time to event\",\n",
        "              \"meta-analysis\", \"model fit\", \"model simulation\", \"monte carlo\", \"multivariate hazard ratio\", \"narrative review\",\n",
        "              \"non-comparative study\", r\"non[\\-\\s]randomised\", r\"non[\\-\\s]randomized\",\n",
        "              \"non-response bias\", \"number of controls per case\", \"odds\", \"odds ratio\", \"outcomes\", \"patients\", \"per capital\", \"placebo\",\n",
        "              \"pooled adjusted odds ratio\", \"pooled aor\", \"pooled odds ratio\", \"pooled or\", \"pooled relative risk\", \"pooled risk ratio\",\n",
        "              \"pooled rr\", \"potential confounders\", \"power\", \"prevalence\", \"prevalence survey\", \"prisma\", \"prospective cohort\",\n",
        "              r\"prospective(?:ly)?\", \"protocol\", \"pseudo-randomised\", \"pseudo-randomized\", \"psychometric evaluation of instrument\",\n",
        "              \"psychometric evaluaton of instrument\", \"publication bias\", \"quasi-randomised\", \"quasi-randomized\", \"questionnaire development\",\n",
        "              \"r-squared\", \"randomisation\", \"randomisation consort\", \"randomised\", \"randomization method\", \"randomized\", \"randomized clinical trial\",\n",
        "              \"randomized controlled trial\", \"rct\", \"receiver-operator curve\", r\"recruit(?:ed|ment)?\", \"registry\", \"registry data\",\n",
        "              \"relative risk\", \"response rate\", \"retrospective\", \"retrospective chart review\", \"retrospective cohort\", \"right-censored\",\n",
        "              \"risk factor analysis\", \"risk factors\", \"risk factors data collection instrument\", \"risk of bias\", \"risk ratio\", \"roc\", \"rr\",\n",
        "              \"search criteria\", \"search strategy\", \"search string\", r\"simulat(?:e|ed|ion)\", \"statistical model\", \"stochastic model\", \"strength\",\n",
        "              \"subjects\", \"surveillance\", \"survey instrument\", \"survival analysis\", \"symptoms\", \"syndromic surveillance\", \"synthetic\",\n",
        "              \"synthetic data\", r\"synthetic data(?:set(?:s)?)?\", \"systematic review\", \"time-to-event analysis\", r\"time[\\-\\s]series\",\n",
        "              r\"time[\\-\\s]varying\", \"tolerability\", \"treatment arm\", \"treatment effect\", \"truncated\", \"weibull\"]\n",
        "\n",
        "    # Sample vocabulary\n",
        "    SAMPLE = [\"articles\", \"cases\", \"children\", \"individuals\", \"men\", \"participants\", \"patients\", \"publications\", \"samples\", \"sequences\",\n",
        "              \"studies\", \"trials\", \"total\", \"women\"]\n",
        "\n",
        "    # Sample methods vocabulary\n",
        "    METHOD = [\"analyse\", \"analyze\", \"ci\", \"clinical\", \"collect\", \"compare\", \"data\", \"database\", \"demographic\", \"enroll\", \"epidemiological\",\n",
        "              \"evidence\", \"findings\", \"hospital\", \"include\", \"materials\", r\"method(?:s)?:?\", \"observe\", \"obtain\", \"perform\", \"publication\",\n",
        "              \"publish\", \"recruit\", r\"result(?:s)?:?\", \"retrieve\", \"review\", \"search\", \"study\", \"studie\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XFn7xZHX1Cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's take the positive sample predicition sentence and parse it\n",
        "sentence=\"Using data from the Truven Health MarketScan Research Database (2010-2017), we identified 12,673 men (age 45-64) with ARDS, of whom 1,189 patients (9.4%) were prescribed α1-AR antagonists in the previous year.\"\n",
        "#Ok what needs to be done to sentence\n",
        "# |load the vocab, tokenize, pass to find"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5xT0emVV3_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's define some functions we will need\n",
        "Vocab\n",
        "from word2number import w2n\n",
        "def extract(sentence, attribute):\n",
        "  size = None\n",
        "  nlp = en_core_web_sm.load()\n",
        "  #nlp = spacy.load(\"en_core_sci_md\")\n",
        "  tokens = nlp(sentence)\n",
        " \n",
        "  #nlp = en_core_web_sm.load()\n",
        "  #Need to NLP tokenize\n",
        "  size = find(tokens, Vocab.SAMPLE)\n",
        "  return size\n",
        "def tonumber(token):\n",
        "  try:\n",
        "      return \"%d\" % w2n.word_to_num(token.replace(\",\", \"\"))\n",
        "  # pylint: disable=W0702\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "  return token\n",
        "def find(tokens, keywords):\n",
        "  matches = [match(token, keywords) for token in tokens]\n",
        "  matches = [match for match in matches if match]\n",
        "\n",
        "  return matches[0][0] if matches else None\n",
        "  \n",
        "def match(token, keywords):\n",
        "  if token.text.lower() in keywords:\n",
        "    return [tonumber(c.text) for c in token.children if isnumber(c)]\n",
        "\n",
        "  return None\n",
        "\n",
        "def isnumber(token):\n",
        "  # Returns true if following conditions are met:\n",
        "  #  - Token POS is a number of it's all digits\n",
        "  #  - Token DEP is in [amod, nummod]\n",
        "  #  - None of the children are brackets (ignore citations [1], [2], etc)\n",
        "  return (token.text.isdigit() or token.pos_ == \"NUM\") and token.dep_ in [\"amod\", \"nummod\"] and not any([c.text == \"[\" for c in token.children])\n",
        "\n",
        "extracted = extract(sentence,\"sample\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0imIrLGMVT6A",
        "colab_type": "code",
        "outputId": "baa34769-32b8-4172-e2ee-5c3e0f80e1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Output the final value from the one sample sentence. \n",
        "extracted"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'12673'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1dJ8il5yrrf",
        "colab_type": "text"
      },
      "source": [
        "#Validation and Enhancement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3MFkVpiK0p2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #Test out a loop for all the predictions and review for where the error\n",
        " count = 0\n",
        "for i in range(1000) :  \n",
        "  if prediction_all[i]:\n",
        "    count = count + 1\n",
        "    #print(prediction_all[i])\n",
        "    print(testsentences[i])\n",
        "    extracted = extracted = extract(df.text[i],\"sample\")\n",
        "    print(extracted)\n",
        "    print(df.loc[i, \"sample\"])\n",
        "    print(\"-----------------------\")\n",
        "print(\"Total:\")\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQvz-SLzT89D",
        "colab_type": "text"
      },
      "source": [
        "#Review all falgged sentences and extract to see bad return Sample Sizes\n",
        "This section will do a systematic review of the results, and note the corner cases to formulate a plan on how to improve accuracy. \n",
        "\n",
        "Notes:\n",
        "\n",
        "\n",
        "\n",
        "# Errors in results:\n",
        "Each sentence below is followed by the extraction number for sample size and the flag it was actually given in the training data. \n",
        "\n",
        "\n",
        "Methods The clinical records, laboratory findings and radiological assessments included chest X-ray or computed tomography were extracted from electronic medical records of 25 died patients with COVID-19 in Renmin Hospital of Wuhan University from Jan 14 to Feb 13, 2020.\n",
        "None\n",
        "\n",
        "Three-hundred and fifty-five COVID-19 patients with were recruited and clinical data were collected from electronic medical records.\n",
        "100\n",
        "\n",
        "Methods A total of 8 274 cases in Wuhan were enrolled in this cross-sectional study during January 20 to February 9, 2020, and were tested for 2019-nCoV using fluorescence quantitative PCR.\n",
        "9\n",
        "\n",
        "Results 112 COVID-19 patients were enrolled in our study.\n",
        "None\n",
        "\n",
        "In total, there were 13 infected evacuees including five asymptomatic individuals as of 16 February 2020.\n",
        "5 - \n",
        "\n",
        "Huang and colleagues 1 only included 59 suspected cases with fever and dry cough, and 41 patients were con firmed to be infected with SARS-CoV-2.\n",
        "59 - technically right?\n",
        "\n",
        "Bad Labelling - this one shouldn't flag imo\n",
        "All countries that had reported at least 15 days of at least 100 total confirmed cases, and that had available data on BCG policy and covariates (median age, gross domestic product per capita, population density, population size, net migration rate, and geographical region) were included (52 countries in total).\n",
        "100\n",
        "\n",
        "315  316  317  318  319  320  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363  364  365  366   367  368  369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416  417  418\n",
        "None - bad flag or bad tagging \n",
        "-----------------------\n",
        "E093392, E093393, E093399, E09341, E093411, E093412, E093413, E093419, E09349, E093491, E093492, E093493, E093499, E09351, E093511, E093512,  E093513, E093519, E093521, E093522, E093523, E093529, E093531, E093532, E093533, E093539, E093541, E093542, E093543, E093549, E093551, E093552,  E093553, E093559, E09359, E093591, E093592, E093593, E093599, E0936, E0937X1, E0937X2, E0937X3, E0937X9, E0939, E0940, E0941, E0942, E0943,  E0944, E0949, E0951, E0952, E0959, E09610, E09618, E09620, E09621, E09622, E09628, E09630, E09638, E09641, E09649, E0965, E0969, E098, E1010,  E1011, E1021, E1022, E1029, E10311, E10319, E10321, E103211, E103212, E103213, E103219, E10329, E103291, E103292, E103293, E103299, E10331,  E103311, E103312, E103313, E103319, E10339, E103391, E103392, E103393, E103399, E10341, E103411, E103412, E103413, E103419, E10349, E103491,  E103492, E103493, E103499, E10351, E103511, E103512, E103513, E103519, E103521, E103522, E103523, E103529, E103531, E103532, E103533, E103539,  E103541, E103542, E103543, E103549, E103551, E103552, E103553, E103559, E10359, E103591, E103592, E103593, E103599, E1036, E1037X1, E1037X2,  E1037X3, E1037X9, E1039, E1040, E1041, E1042, E1043, E1044, E1049, E1051, E1052, E1059, E10610, E10618, E10620, E10621, E10622, E10628, E10630,  E10638, E10641, E10649, E1065, E1069, E108, E1100, E1101, E1110, E1111, E1121, E1122, E1129, E11311, E11319, E11321, E113211, E113212, E113213,  E113219, E11329, E113291, E113292, E113293, E113299, E11331, E113311, E113312, E113313, E113319, E11339, E113391, E113392, E113393, E113399,  E11341, E113411, E113412, E113413, E113419, E11349, E113491, E113492, E113493, E113499, E11351, E113511, E113512, E113513, E113519, E113521,  E113522, E113523, E113529, E113531, E113532, E113533, E113539, E113541, E113542, E113543, E113549, E113551, E113552, E113553, E113559, E11359,  E113591, E113592, E113593, E113599, E1136, E1137X1, E1137X2, E1137X3, E1137X9, E1139, E1140, E1141, E1142, E1143, E1144, E1149, E1151, E1152,  E1159, E11610, E11618, E11620, E11621, E11622, E11628, E11630, E11638, E11641, E11649, E1165, E1169, E118, E1300, E1301, E1310, E1311, E1321,  E1322, E1329, E13311, E13319, E13321, E133211, E133212, E133213, E133219, E13329, E133291, E133292, E133293, E133299, E13331, E133311, E133312,  E133313, E133319, E13339, E133391, E133392, E133393, E133399, E13341, E133411, E133412, E133413, E133419, E13349, E133491, E133492, E133493,  E133499, E13351, E133511, E133512, E133513, E133519, E133521, E133522, E133523, E133529, E133531, E133532, E133533, E133539, E133541, E133542,  E133543, E133549, E133551, E133552, E133553, E133559, E13359, E133591, E133592, E133593, E133599, E1336, E1337X1, E1337X2, E1337X3, E1337X9,  E1339, E1340, E1341, E1342, E1343, E1344, E1349, E1351, E1352, E1359, E13610, E13618, E13620, E13621, E13622, E13628, E13630, E13638, E13641,  E13649 , E1365, E1369, E138.', E093392, E093393, E093399, E09341, E093411, E093412, E093413, E093419, E09349, E093491, E093492, E093493, E093499, E09351, E093511, E093512,  E093513, E093519, E093521, E093522, E093523, E093529, E093531, E093532, E093533, E093539, E093541, E093542, E093543, E093549, E093551, E093552,  E093553, E093559, E09359, E093591, E093592, E093593, E093599, E0936, E0937X1, E0937X2, E0937X3, E0937X9, E0939, E0940, E0941, E0942, E0943,  E0944, E0949, E0951, E0952, E0959, E09610, E09618, E09620, E09621, E09622, E09628, E09630, E09638, E09641, E09649, E0965, E0969, E098, E1010,  E1011, E1021, E1022, E1029, E10311, E10319, E10321, E103211, E103212, E103213, E103219, E10329, E103291, E103292, E103293, E103299, E10331,  E103311, E103312, E103313, E103319, E10339, E103391, E103392, E103393, E103399, E10341, E103411, E103412, E103413, E103419, E10349, E103491,  E103492, E103493, E103499, E10351, E103511, E103512, E103513, E103519, E103521, E103522, E103523, E103529, E103531, E103532, E103533, E103539,  E103541, E103542, E103543, E103549, E103551, E103552, E103553, E103559, E10359, E103591, E103592, E103593, E103599, E1036, E1037X1, E1037X2,  E1037X3, E1037X9, E1039, E1040, E1041, E1042, E1043, E1044, E1049, E1051, E1052, E1059, E10610, E10618, E10620, E10621, E10622, E10628, E10630,  E10638, E10641, E10649, E1065, E1069, E108, E1100, E1101, E1110, E1111, E1121, E1122, E1129, E11311, E11319, E11321, E113211, E113212, E113213,  E113219, E11329, E113291, E113292, E113293, E113299, E11331, E113311, E113312, E113313, E113319, E11339, E113391, E113392, E113393, E113399,  E11341, E113411, E113412, E113413, E113419, E11349, E113491, E113492, E113493, E113499, E11351, E113511, E113512, E113513, E113519, E113521,  E113522, E113523, E113529, E113531, E113532, E113533, E113539, E113541, E113542, E113543, E113549, E113551, E113552, E113553, E113559, E11359,  E113591, E113592, E113593, E113599, E1136, E1137X1, E1137X2, E1137X3, E1137X9, E1139, E1140, E1141, E1142, E1143, E1144, E1149, E1151, E1152,  E1159, E11610, E11618, E11620, E11621, E11622, E11628, E11630, E11638, E11641, E11649, E1165, E1169, E118, E1300, E1301, E1310, E1311, E1321,  E1322, E1329, E13311, E13319, E13321, E133211, E133212, E133213, E133219, E13329, E133291, E133292, E133293, E133299, E13331, E133311, E133312,  E133313, E133319, E13339, E133391, E133392, E133393, E133399, E13341, E133411, E133412, E133413, E133419, E13349, E133491, E133492, E133493,  E133499, E13351, E133511, E133512, E133513, E133519, E133521, E133522, E133523, E133529, E133531, E133532, E133533, E133539, E133541, E133542,  E133543, E133549, E133551, E133552, E133553, E133559, E13359, E133591, E133592, E133593, E133599, E1336, E1337X1, E1337X2, E1337X3, E1337X9,  E1339, E1340, E1341, E1342, E1343, E1344, E1349, E1351, E1352, E1359, E13610, E13618, E13620, E13621, E13622, E13628, E13630, E13638, E13641,  E13649 , E1365, E1369, E138.\n",
        "None - bad flagging or bad data tagging \n",
        "-----------------------\n",
        "12e14 A similar finding was observed in two recent non-peer-reviewed studies: one study with 1099 patients from 552 hospitals in 31 provinces in China, in which the median age was 47.0 years, and 55.1% of the patients were between the ages of 15e49 years; and a second study that included 4021 confirmed cases in 30 provinces of China, in which the mean age was 49 years and 50.7% of patients were between the ages of 20e50 years.\n",
        "2 - bad tagging if it is flagging. It's talking about another report. \n",
        "-----------------------\n",
        "229 The nasal/throat/anal swab samples and mainly tissue compartments collected from 230 infected monkeys were tested for SARS-CoV-2 RNA by quantitative real-time reverse 231 author/funder.\n",
        "None - this study was related to monkeys and monkeys wasn't in vocabulary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIgrwzrkEdJQ",
        "colab_type": "text"
      },
      "source": [
        "#New Method for finding the sample size in the sentence \n",
        "Issues:  \n",
        "\n",
        "*   Only works when single token - Will improve further NLP Parsing \n",
        "* Bad tagging or bad sentence inference classification - Need to improve quality and accuracy on sentence classification. \n",
        "\n",
        "What can be done about these issues in the future? \n",
        "\n",
        "New Method for increasing accuracy accross multi word \n",
        "\n",
        "Training accuracy improvements could be gained from:\n",
        "Increase in good training data\n",
        "Add spacy tags as a feature vector\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYmcxVQzFbh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add in all w2v conversion first per token. Then put all the sequential numbers today. Then send to the matching and child check functions. \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}